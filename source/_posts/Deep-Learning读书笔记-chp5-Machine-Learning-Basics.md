title: Deep Learning读书笔记 chp6-Deep Feedforward Networks
author: nullfan
date: 2020-02-20 18:48:40
tags:
---
# 如何捕捉非线性关系$\phi$  
1、选取一个非常通用的$\phi$，例如RBF核使用的无限维度的$\phi$。使用这样的通常会有足够的能力来适应训练集，但通常不会取得很好的泛化误差。非常通用的特征映射通常是基于局部平滑性，没有引入足够的先验的偏置的信息  
2、人工映射$\phi$。在深度学习兴起之前，自然语言、图像处理、语音识别领域做了分别做了很多努力，但这些对应的编映射的迁移能力很差，没有通用性  
3、深度学习采用的方式是学习这样的$\phi$。这样的一个好处是，我们只需要定义一个宽泛的函数簇，交给神经网络去学习，而不是从找到一个很精确的函数簇  

choose the optimizer
the cost function
the form of output
activation functions

# 激活函数
## relu
在现代神经网络中，默认的激活函数一般选取Relu（rectified linear unit）。使用Relu可以获得非线性变换。由于Relu非常接近线性函数，可以保留许多线性函数使用随机梯度下降的优势，同时也保留了线性函数泛化好的特性。Relu在其定义域中的一半都是线性函数，这使得在其激活（active）时，其导数都很大。
## sigmod
Sigmod函数$g(z) = /sigma (z)$，当$z$为比较的负数的时候，函数本身值会非常小；并且只有函数值只有$z$在0附近的时候才非常敏感，所以sigmod函数当做前馈神经网络隐层的激活函数是不推荐的。sigmod在某些特定的时刻可以作为输出层，比如当损失函数可以抵消simod函数的saturation时。
Sigmod在非前馈神经网络通常使用得更加频繁。例如RNN、autoencoders等神经网络，这些神经网络排除了线性模型，所以还是会更多的使用sigmod函数尽管依然存在saturation。  
## 某些层可以使单纯的线性函数  
单纯使用线性函数可以起到压缩神经网络的效果。考虑一个有$n$个输入和$p$个输出的神经网络，如果直接映射，需要np个参数；如果中间加上一层的输出有$q$个参数，那么总共是需要$(n + p)q$个参数。对于比较小的$q$，可以起到很好的压缩空间的效果。这可以理解为将输入映射到一个低维空间，但通常很小的低维空间也可以表示很多原始的信息。

# 损失函数的选取  
损失函数的梯度必须足够大以便于给学习算法提供一些指导。  
negative log-likelihood可以帮助很多模型解决梯度过小的问题。  
MSE和MAE不适合作为损失函数，因为他们和其他输出层和这些损失函数一起的时候，通常会导致很小的梯度。 

# 神经网络架构的设计  
更深的神经网络通常每层需要的单元和参数更少，泛化能力更强，但也更难被优化。  
屏幕快照 2020-02-23 下午10.28.00
![upload successful](/images/pasted-65.png)  

使用更深的神经网络通常基于这样一个信念：我们需要学习的函数是由很多简单的函数组成的。
title: 趣头条准备
author: nullfan
date: 2019-12-15 22:57:37
tags:
---
## lr和svm的区别  
#### 联系
* 都是线性模型，一般都处理二分类问题  
* 都可以加正则项  
* LR和SVM都可以用来做非线性分类，只要加核函数就好。
#### 区别  
* 从目标函数来看，lr使用的是logistical loss，svm使用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
* 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
* SVM不直接依赖数据分布，而LR则依赖，因为SVM只与支持向量那几个点有关系，而LR和所有点都有关系。  
* SVM本身是结构风险最小化模型，而LR是经验风险最小化模型  
* 在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。

## id3 c4.5等
https://zhuanlan.zhihu.com/p/34534004


## bn & dropout
batch Normalization优势  

（1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度    
BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。  
（2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定  

![upload successful](/images/pasted-53.png)

![upload successful](/images/pasted-54.png)  

（3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题
在不使用BN层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；    

（4）BN具有一定的正则化效果

在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。


dropout的作用  

![upload successful](/images/pasted-55.png)  

https://zhuanlan.zhihu.com/p/33173246  



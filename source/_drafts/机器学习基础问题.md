title: 机器学习基础问题
author: nullfan
date: 2019-11-19 16:13:44
tags:
---
## 方差和偏差  
https://www.zhihu.com/question/27068705/answer/82132134

![upload successful](/images/pasted-26.png)

![upload successful](/images/pasted-27.png)

![upload successful](/images/pasted-28.png)

* https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a  
## L1和L2范式  
https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models  

###### why does a small L1 norm give a sparse solution? 
* By L1 regularization, you essentially make the vector x smaller (sparse), as most of its components are useless (zeros), and at the same time, the remaining non-zero components are very “useful”.  
* In the regularized sparse solution, you ensure that each component of the vector x is very capable. Each component must capture some useful feature or pattern of the data.  
###### why does a sparse solution avoid over-fitting?  
But again, the probability of touching a tip is very high. I guess this is even more true for high dimension, real-world problems. As when your coordinate system has more axises, your L1 norm shape should have more spikes or tips. It must look like a cactus or a hedgehog! I can’t imagine.  

###### what does regularization do really?   
* In conclusion, over-fitting is a problem you see when your machine learning model is too large (has too many parameters) comparing to your available training data. In this case, the model tends to remember all training cases including noisy to achieve better training score. To avoid this, regularization is applied to the model to (essentially) reduce its size. One way of regularization is making sure the trained model is sparse so that the majority of it’s components are zeros. Those zeros are essentially useless, and your model size is in fact reduced. 

## 迪杰斯特拉(Dijkstra)算法描述及其正确性证明
https://blog.csdn.net/softee/article/details/39034129

## 逻辑回归公式推导  
https://zhuanlan.zhihu.com/p/44591359  

![upload successful](/images/pasted-24.png)

![upload successful](/images/pasted-25.png)
## PCA
https://zhuanlan.zhihu.com/p/21580949
## 数据不均衡的解决方法
* 欠采样  
* 过采样   
* 损失函数（代价敏感的损失函数）  
* 组合集成学习： 组合/集成学习：例如正负样本比例1:100，则将负样本分成100份，正样本每次有放回采样至与负样本数相同，然后取100次结果进行平均。  

## 经典聚类算法  
###### 层次聚类  
###### K-means聚类
###### DBSAN聚类(具有噪声的基于密度的聚类方法)
###### Mean-Shift聚类
## GBDT和XGBoost的区别？
* xgboost独特的地方：1、使用了二阶导数 2、在正则项引入树的节点树，控制树的复杂度
* xgboost的slides:
https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf
* GBDT的slides:  
http://wepon.me/files/gbdt.pdf
###### 二者区别：
* 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。  
* 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。  
* xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
* Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
* 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。

* **xgboost工具支持并行。**boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。**xgboost的并行是在特征粒度上的。**我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
* 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

作者：wepon
链接：https://www.zhihu.com/question/41354392/answer/98658997
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
作者：wepon
链接：https://www.zhihu.com/question/41354392/answer/98658997
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
作者：wepon
链接：https://www.zhihu.com/question/41354392/answer/98658997
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
title: 需要准备的问题
author: nullfan
date: 2019-11-11 10:49:53
tags:
---
## 图模型  
SDNE：同时考虑一阶、二阶信息
## word2vec  
## xgboost  
http://wepon.me/files/gbdt.pdf
https://homes.cs.washington.edu/~tqchen/data/pdf/BoostedTree.pdf

https://snaildove.github.io/2018/10/02/get-started-XGBoost/
https://zhuanlan.zhihu.com/p/87885678
CART ID3


![upload successful](/images/pasted-29.png)


![upload successful](/images/pasted-31.png)


![upload successful](/images/pasted-32.png)  


![upload successful](/images/pasted-33.png)

![upload successful](/images/pasted-34.png)


![upload successful](/images/pasted-35.png)

#### lightgbm改进  

![upload successful](/images/pasted-36.png)


![upload successful](/images/pasted-37.png)


![upload successful](/images/pasted-38.png)


## ANN & BSFG大规模向量检索  
## attention
https://zhuanlan.zhihu.com/p/47282410
https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
https://jalammar.github.io/illustrated-transformer/
https://www.youtube.com/watch?fdav=53YvP6gdD7U&feature=youtu.be&t=335
## 基础线性模型

#### FTRL  
***在线最优化求解**： https://github.com/wzhe06/Ad-papers/blob/master/Optimization%20Method/%E5%9C%A8%E7%BA%BF%E6%9C%80%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3(Online%20Optimization)-%E5%86%AF%E6%89%AC.pdf  

![upload successful](/images/pasted-39.png)

![upload successful](/images/pasted-40.png)
* training memory saving

![upload successful](/images/pasted-41.png)

###### 优化点：
* **每一维权重设置各自的学习率**:因为不同维度在样本中出现的次数不同，对于出现次数少的，需要增大其学习率，让其更收敛，而对于出现次数比较多的维度，学习比较充分，需要减少其学习率；  
* ** 出现次数很少的特征删除**：初始候选特征列表为空，对于不在候选特征列表的特征，每个特征按照p的概率丢弃。加入候选特征集合的平均期望出现次数为1/p  
* **使用更短的字节编码浮点数** : 将浮点数字节从32位改为16位  
* ** 训练很多相似的模型** : 
* ** 对数据进行采样** : 对负样本按照概率p进行采样，同时在loss中将负样本的loss乘以(1/p)，保证结果没有bias

####   FTRL成型史
* ** Truncated Gradient（TD）:** ：每轮迭代后直接对权值很小的截断，这很粗暴，使用截断梯度法

![upload successful](/images/pasted-42.png)
采用分段截断法
* ** FOBOS (Forward-Backward Splitting) :** ：
先根据次梯度做梯度下降，然后再做投影  

![upload successful](/images/pasted-43.png)
第一项用于投影后的w和梯度下降的w距离不太远，第二项正则用于保证稀疏性  

* ** RDA(regulized dual averaging): **

![upload successful](/images/pasted-44.png)

![upload successful](/images/pasted-45.png)


![upload successful](/images/pasted-46.png)


![upload successful](/images/pasted-47.png)

## 近邻检索问题  
* ** LSH **
:https://blog.csdn.net/hero_fantao/article/details/70245387  
* **annoy :** https://segmentfault.com/a/1190000013713357
* ** learning to hash :**

## word2vec  
https://blog.csdn.net/mytestmy/article/details/26961315  
https://cloud.tencent.com/developer/article/1135859
* Skip-gram:如果是用一个词语作为输入，来预测它周围的上下文  
* CBOW 模型:拿一个词语的上下文作为输入，来预测这个词语本身    

###  hierarchical softmax
本质是把 N 分类问题变成 log(N)次二分类
https://www.quora.com/What-is-hierarchical-softmax
https://yinwenpeng.wordpress.com/2013/09/26/hierarchical-softmax-in-neural-network-language-model/
https://www.cnblogs.com/pinard/p/7243513.html

### negative sampling  
本质是预测总体类别的一个子集

![upload successful](/images/pasted-8.png)

![upload successful](/images/pasted-9.png)

#### 演示工具
https://ronxin.github.io/wevi/


http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=word2vec#pyspark.ml.feature.Word2Vec

* word2vec中的数学原理：https://www.cnblogs.com/peghoty/p/3857839.html

## graph embedding  
* NE 的中心思想就是找到一种映射函数，该函数将网络中的每个节点转换为低维度的潜在表示  
node embedding需要保持的特性:
* Choice of property
* Scalability  
* Dimensionality of the embedding  
## Alias Method:时间复杂度O(1)的离散采样方法
* https://zhuanlan.zhihu.com/p/54867139
* https://blog.csdn.net/haolexiao/article/details/65157026  
## auto encoder introduction  
https://www.jeremyjordan.me/autoencoders/
## 常见的embedding方法  
### Factorization based
### Random Walk based
### Deep Learning based

## FM

![upload successful](/images/pasted-48.png)  
## NLP基础算法
###### LDA
##### PLSA

## youtube视频推荐特征提取的例子  
* 历史搜索query：把历史搜索的query分词后的token按照权重进行加权平均，能够反映用户整体搜索的历史状态
* 人口统计信息:  性别、年龄、地域等
* 其他上下文信息:  设备、登录状态等
* 视频上传时间    

![upload successful](/images/pasted-51.png)

* 使用更广的数据:不仅仅使用推荐场景的数据进行训练，其他场景比如搜索等的数据也要用到，这样也能为推荐场景提供一些explore。  
* 为每个用户生成数量固定的训练样本:我们在实际中发现的一个practical lessons，如果为每个用户固定样本数量上限，平等的对待每个用户，避免loss被少数active用户domanate，能明显提升线上效果。  
* 抛弃序列信息:我们在实现时尝试的是去掉序列信息，对过去观看视频/历史搜索query的embedding向量进行加权平均。这点其实违反直觉，可能原因是模型对负反馈没有很好的建模。  
* 不对称的共同浏览问题:

![upload successful](/images/pasted-52.png) 

#### 特征工程:
* 用户和商品本身或者相似商品之间的交互  
比如我们度量用户对商品的喜爱程度，可以考虑用户与该视频对应频道的交互情况  
* 数量特征：浏览该频道的特征  
* 最近一次浏览该频道距离现在的时间   
把Matching阶段的信息传播到Ranking阶段同样能很好的提升效果，比如推荐来源和所在来源的分数。

##### Embedding Categorical Features  
NN更适合处理连续特征，因此稀疏的特别是高基数空间的离散特征需要embedding到稠密的向量中。每个维度（比如query/user_id）都有独立的embedding空间，一般来说空间的维度基本与log(去重后值得数量)相当。实际并非为所有的id进行embedding，比如视频id，只需要按照点击排序，选择top N视频进行embedding，其余置为0向量。而对于像“过去点击的视频”这种multivalent特征，与Matching阶段的处理相同，进行加权平均即可。    

#### Normalizing Continuous Features

众所周知，NN对输入特征的尺度和分布都是非常敏感的，实际上基本上除了Tree-Based的模型（比如GBDT/RF），机器学习的大多算法都如此。我们发现归一化方法对收敛很关键，推荐一种排序分位归一到[0,1]区间的方法，即[公式]，累计分位点。

除此之外，我们还把归一化后的[公式]的根号[公式]和平方[公式]作为网络输入，以期能使网络能够更容易得到特征的次线性（sub-linear）和（super-linear）超线性函数。

## 损失函数  
https://gombru.github.io/2018/05/23/cross_entropy_loss/  
https://gombru.github.io/2019/04/03/ranking_loss/


![upload successful](/images/pasted-56.png)  


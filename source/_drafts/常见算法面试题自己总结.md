title: 常见算法面试题自己总结
author: nullfan
date: 2019-11-14 15:05:58
tags:
---
## 激活函数
* 激活函数的作用：引入非线性变换； 如果没有激活函数，层数再多，也只是线性变换  
* sigmoid函数

![upload successful](/images/pasted-13.png)

![upload successful](/images/pasted-10.png)
用户二分类，x靠近0时，一点点小小的值也能引起y很大的变化；  
x很大时，x的变化引起y的变化很小，反向传播几乎起不到梯度更新的作用；  
sigmoid函数只能得到正值，在很多情况下不适用  

* tanh函数

![upload successful](/images/pasted-14.png)

![upload successful](/images/pasted-11.png)
对sigmoid函数进行改进，取值范围在(-1,1)之间，其他的性质和sigmoid函数一样  

* relu函数  

![upload successful](/images/pasted-15.png)
输入为负值时，输出为0，只有部分神经网络会被更新；   
输出为0的神经元永远得不到更新，变成死神经元  
* leaky relu函数  

![upload successful](/images/pasted-16.png)  

![upload successful](/images/pasted-17.png)  
解决了死神经元的问题  
* softmax函数（归一化指数函数）  

![upload successful](/images/pasted-12.png)  


![upload successful](/images/pasted-18.png)  


## 梯度下降  
* 全局梯度下降  
a、全部样本的梯度能更好的代表样本总体，更准确的朝着优化目标前进  
b、不是凸函数的情况，不能保证达到最优解  
c、计算速度比较慢
* 随机梯度下降  
a、每次优化一条样本，迭代速度大大加快  
b、某条样本的损失最小，并不能代表全局的损失最小，可能无法达到全局最优  
* 批梯度下降  
a、通过矩阵运算，每次在一个batch上优化参数并不比单个数据慢太多  
b、每次使用一个batch进行更新，可以大大缩减所需要的迭代轮数，同时可以使收敛的结果更加接近梯度下降的结果
  
## pool的求导  
* mean_pooling : mean pooling的前向传播就是把一个patch中的值求取平均来做pooling，那么反向传播的过程也就是把某个元素的梯度等分为n份分配给前一层，这样就保证池化前后的梯度（残差）之和保持不变  
* max_pooling : max_pooling正向传播时取周围4个像素的最大值保留，其余的值丢弃，所以反向传播时将梯度对应回最大值的位置，其他位置取0  
## 浅层网络和深层网络的区别  
* 神经网络中，权重参数是给数据做线性变换，而激活函数给数据带来的非线性变换。增加某一层神经元数量是在增加线性变换的复杂性，而增加网络层数是在增加非线性变换的复杂性。
* 理论上来说，浅层神经网络就能模拟任何函数，但需要巨大的数据量，而深层神经网络可以用更少的数据量来学习到更好的拟合。  
## 什么是过拟合以及防止过拟合的方法  
* 过拟合是指模型学习能力过于强大，把数据部分不太一般的特性都学到了，导致泛化性很差  
###  避免过拟合的方法
* L2正则  

![upload successful](/images/pasted-20.png)
* L1正则  

![upload successful](/images/pasted-21.png)
* Dropout  
* 增大数据量  
* early stop  
* batch normalization

## soft max loss和cross entry loss  

![upload successful](/images/pasted-22.png)  
## RNN & LSTM  
* RNN存在的问题：梯度消失  
* LSTM:  

![upload successful](/images/pasted-23.png)  
## relu不适合rnn，却适合lstm
https://www.zhihu.com/question/61265076/answer/186347780